{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning With Python: Linear Regression With One Variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example\n",
    "<h3 style=\"color:purple\">Sample problem of predicting home price in monroe, new jersey (USA)</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below table represents current home prices in monroe township based on square feet area, new jersey\n",
    "\n",
    "We're going to use `sklearn` to build our model, you can find the library tool in here [linear model](https://scikit-learn.org/stable/modules/linear_model.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "glob.glob('*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import linear_model\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read csv file by pandas\n",
    "df = pd.read_csv('homeprices.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing the distribution of housing prices\n",
    "# we use the scatter chart in here\n",
    "%matplotlib inline\n",
    "plt.xlabel('area')\n",
    "plt.ylabel('price')\n",
    "plt.scatter(df.area,df.price,color='red',marker='+')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We try to remove the price value from dataframe table and save it to area_df\n",
    "area_df = df.drop('price',axis='columns')\n",
    "area_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For our original file, the data has not changed. We can still call the price data through df\n",
    "price = df.price\n",
    "price"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We will use sklearn.linear_model to train a linear regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare the training data X = area and Y = price\n",
    "X_train = area_df\n",
    "y_train = price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create linear regression object\n",
    "reg = linear_model.LinearRegression()\n",
    "reg.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### After training, we use the trained model to make predictions\n",
    "**(1) Predict price of a home with area = 3300 sqr ft**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# The input value is area, \n",
    "# The trained model will give us the corresponding house price.\n",
    "\n",
    "reg.predict([[3300]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the parameters of linear regression\n",
    "$$y = m*x + b$$\n",
    "\n",
    "Where **m** is the coefficient and **b** is the intercept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the m (coefficient) or slope\n",
    "reg.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# check the b (intercept)\n",
    "reg.intercept_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Y = m * X + b (m is coefficient and b is intercept)\n",
    "135.78767123 * 3300 + 180616.43835616432"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(1) Predict price of a home with area = 5000 sqr ft**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg.predict([[5000]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing the training set\n",
    "plt.scatter(X_train, y_train, color = 'red')\n",
    "# plt.scatter(X_train, regressor.predict(X_train), color = 'blue')\n",
    "plt.plot(X_train, reg.predict(X_train), color = 'blue')\n",
    "plt.title('<Your Name> + Area vs price')\n",
    "plt.xlabel('Area')\n",
    "plt.ylabel('price')\n",
    "plt.grid(alpha=0.4)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What Makes Linear Regression Understandable? \n",
    "Linear regression is easy to understand because it generates a straight line that models the relationship between variables. The method uses the cost function, specifically [mean squared error](https://en.wikipedia.org/wiki/Mean_squared_error) (MSE), to calculate the average distance of each data point from the line. By iteratively adjusting the line's slope and intercept, linear regression minimizes this distance, ensuring the line fits the data as accurately as possible.\n",
    "\n",
    "Intuitively, MSE represents an aggregation of the distances between point's actual y value and what a hypothesis function $h_\\theta(x)$ predicted it would be.  \n",
    "\n",
    "\\begin{align}\n",
    "h_\\theta(x) & = \\theta_0 + \\theta_1x_1 \\\\\n",
    "\\end{align}\n",
    "\n",
    "\n",
    "That hypothesis function and the cost function $J(\\theta)$ are defined as\n",
    "\\begin{align}\n",
    "J(\\theta) & = \\frac{1}{2m}\\sum\\limits_{i = 1}^{m}(h_\\theta(x^{(i)}) - y^{(i)})^2\n",
    "\\end{align}\n",
    "\n",
    "where $\\theta$ is a vector of feature weights, $x^{(i)}$ is the ith training example, $y^{(i)}$ is that example's y value, and $x_j$ is the value for its jth feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def compute_cost(X, y, theta):\n",
    "    return np.sum(np.square(np.dot(X, theta) - y)) / (2 * len(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before computing the cost with an initial guess for $\\theta$, a column of 1s is prepended onto the input data.  This allows us to vectorize the cost function, as well as make it usable for multiple linear regression later.  This first value $\\theta_0$ now behaves as a constant in the cost function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta = np.zeros(2) # we need two parameters -  Slope and Intercept\n",
    "\n",
    "X = np.column_stack((np.ones(len(area_df)), area_df/1000))\n",
    "y = price/1000\n",
    "cost = compute_cost(X, y, theta)\n",
    "\n",
    "print('theta:', theta)\n",
    "print('cost:', cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example for column_stack\n",
    "import numpy as np\n",
    "a = np.array((1,2,3))\n",
    "b = np.array((2,3,4))\n",
    "\n",
    "np.column_stack((a,b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll now minimize the cost using the [gradient descent](https://en.wikipedia.org/wiki/Gradient_descent) algorithm.  Intuitively, gradient descent takes small, linear hops down the slope of a function in each feature dimension, with the size of each hop determined by the partial derivative of the cost function with respect to that feature and a learning rate multiplier $\\alpha$.  If tuned properly, the algorithm converges on a global minimum by iteratively adjusting feature weights $\\theta$ of the cost function, as shown here for two feature dimensions:\n",
    "\n",
    "\\begin{align}\n",
    "\\theta_0 & := \\theta_0 - \\alpha\\frac{\\partial}{\\partial\\theta_0} J(\\theta_0,\\theta_1) \\\\\n",
    "\\theta_1 & := \\theta_1 - \\alpha\\frac{\\partial}{\\partial\\theta_1} J(\\theta_0,\\theta_1) \n",
    "\\end{align}\n",
    "\n",
    "The update rule each iteration then becomes:\n",
    "\n",
    "\\begin{align}\n",
    "\\theta_0 & := \\theta_0 - \\alpha\\frac{1}{m} \\sum_{i=1}^m (h_\\theta(x^{(i)})-y^{(i)}) \\\\\n",
    "\\theta_1 & := \\theta_1 - \\alpha\\frac{1}{m} \\sum_{i=1}^m (h_\\theta(x^{(i)})-y^{(i)})x_1^{(i)} \\\\\n",
    "\\end{align}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the gradient descent function\n",
    "def gradient_descent(X, y, alpha, iterations):\n",
    "    \"\"\"\n",
    "    Performs gradient descent to minimize the cost function and find the optimal parameters.\n",
    "    \n",
    "    Parameters:\n",
    "    X: ndarray\n",
    "        Input feature matrix, where rows are samples and columns are features.\n",
    "    y: ndarray\n",
    "        Target values corresponding to the input data.\n",
    "    alpha: float\n",
    "        Learning rate to control the step size of parameter updates.\n",
    "    iterations: int\n",
    "        Number of iterations to run gradient descent.\n",
    "\n",
    "    Returns:\n",
    "    theta: ndarray\n",
    "        Optimized parameters for the linear model.\n",
    "    \"\"\"\n",
    "    theta = np.zeros(2)  # Initialize theta with zeros (2 parameters for simple linear regression)\n",
    "    m = len(y)           # Number of training examples\n",
    "\n",
    "    # Iterate to update theta\n",
    "    for i in range(iterations):\n",
    "        # Update theta[0] (intercept term) using the gradient of the cost function\n",
    "        t0 = theta[0] - (alpha / m) * np.sum(np.dot(X, theta) - y)\n",
    "        \n",
    "        # Update theta[1] (slope term) using the gradient of the cost function\n",
    "        t1 = theta[1] - (alpha / m) * np.sum((np.dot(X, theta) - y) * X[:, 1])\n",
    "        \n",
    "        # Update theta as a new array with the computed values\n",
    "        theta = np.array([t0, t1])\n",
    "\n",
    "    return theta\n",
    "\n",
    "# Number of iterations for gradient descent\n",
    "iterations = 5000\n",
    "\n",
    "# Learning rate for gradient descent\n",
    "alpha = 0.1\n",
    "\n",
    "# Call the gradient descent function to compute optimized parameters\n",
    "theta = gradient_descent(X, y, alpha, iterations)\n",
    "\n",
    "# Compute the cost using the optimized parameters\n",
    "cost = compute_cost(X, y, theta)\n",
    "\n",
    "# Print the results\n",
    "print(\"theta:\", theta)  # Optimized values for theta (intercept and slope)\n",
    "print('cost:', compute_cost(X, y, theta))  # Final cost value to evaluate the fit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(area_df, price, marker='x', color='red')\n",
    "samples = np.linspace(np.min(area_df), np.max(area_df))\n",
    "plt.plot(samples, theta[0]*1000 + theta[1] * samples,color = 'blue')\n",
    "\n",
    "# plt.plot(X_train, reg.predict(X_train), color = 'Green')\n",
    "plt.title('<Yourname> + Area vs price')\n",
    "plt.xlabel('Area')\n",
    "plt.ylabel('price')\n",
    "plt.grid(alpha=0.4)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
