{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c649b02a",
   "metadata": {},
   "source": [
    "# Pokémon Data Classification: Logistic Regression vs MLP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89920876",
   "metadata": {},
   "source": [
    "## Objective\n",
    "This notebook demonstrates how to classify Pokémon as legendary or not using two machine learning models:\n",
    "- Logistic Regression\n",
    "- Multi-Layer Perceptron (MLP)\n",
    "\n",
    "We will compare the performance of both models using metrics such as accuracy, precision, recall, F1-score, and AUC-ROC."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7aea16b",
   "metadata": {},
   "source": [
    "## Step 1: Load and Explore the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "200d8967",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load the Pokémon dataset\n",
    "file_path = 'pokemon.csv'  # Ensure this file is in the same directory as the notebook\n",
    "pokemon_data = pd.read_csv(file_path)\n",
    "\n",
    "# Display the first few rows\n",
    "pokemon_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41159a0f",
   "metadata": {},
   "source": [
    "## Step 2: Data Preprocessing\n",
    "### 2.1 Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "228ac0aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a derived feature: attack_to_defense_ratio\n",
    "pokemon_data['attack_to_defense_ratio'] = pokemon_data['attack'] / pokemon_data['defense']\n",
    "\n",
    "# Select features and target\n",
    "features = ['attack', 'defense', 'sp_attack', 'sp_defense', 'speed', 'total_bs',\n",
    "            'capture_rt', 'attack_to_defense_ratio', 'type']\n",
    "target = 'legendary'\n",
    "X = pokemon_data[features]\n",
    "y = pokemon_data[target]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "825347e0-97fa-423e-8318-fe669b1fa270",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.preprocessing import MinMaxScaler, Normalizer, MaxAbsScaler\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f23f4262-3ca7-4c4f-a1ad-4b3f3e69300b",
   "metadata": {},
   "source": [
    "### 2.2 Example for Scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe5127d1-6abf-4bcc-a34f-5c509556c91b",
   "metadata": {},
   "source": [
    "### MinMaxScaler Example "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "862d07c7-b4cb-4232-96da-27d0d2a07f38",
   "metadata": {},
   "outputs": [],
   "source": [
    "X.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f98b78f6-160a-4da6-8aa7-7eae1c7e3f45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Min-Max Scaling\n",
    "# Select features and target\n",
    "features_digit = ['attack', 'defense', 'sp_attack', 'sp_defense', 'speed', 'total_bs',\n",
    "            'capture_rt']\n",
    "target = 'legendary'\n",
    "X_example = pokemon_data[features_digit]\n",
    "y_target = pokemon_data[target]\n",
    "\n",
    "min_max_scaler = MinMaxScaler()\n",
    "X_min_max_scaled = pd.DataFrame(min_max_scaler.fit_transform(X_example), columns=features_digit)\n",
    "print(\"Min-Max Scaled Data:\")\n",
    "print(X_min_max_scaled.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c0a90aa-95c9-4eac-a939-aa4da684830b",
   "metadata": {},
   "source": [
    "### Normalizer Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "788dbdc5-547a-45f7-b8a6-cb9cd8b72bea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalization with L1 Norm\n",
    "l1_normalizer = Normalizer(norm='l1')\n",
    "X_l1 = l1_normalizer.fit_transform(X_example)\n",
    "X_normalized_l1 = pd.DataFrame(X_l1, columns=features_digit)\n",
    "print(\"\\nL1 Normalized Data:\\n\")\n",
    "print(X_normalized_l1.head(5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efe2a87e-a221-444b-906f-d3a0d10d3bee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalization with L2 Norm\n",
    "l2_normalizer = Normalizer(norm='l2')\n",
    "X_l2 = l2_normalizer.fit_transform(X_example)\n",
    "X_normalized_l2 = pd.DataFrame(X_l2, columns=features_digit)\n",
    "print(\"\\nL2 Normalized Data:\\n\")\n",
    "print(X_normalized_l2.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dbcb335-60ed-48c5-8073-aff70a913e17",
   "metadata": {},
   "source": [
    "### MaxAbsScaler(Maximum Absolute scaler) Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37e68b2a-ad9c-4f9a-848d-4747745de900",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Maximum Absolute Scaling\n",
    "max_abs_scaler = MaxAbsScaler()\n",
    "X_max_abs_scaled = pd.DataFrame(max_abs_scaler.fit_transform(X_example), columns=features_digit)\n",
    "print(\"\\nMax-Abs Scaled Data:\")\n",
    "print(X_max_abs_scaled.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39c6c1c2-a906-46e5-b017-8eac2ce9b581",
   "metadata": {},
   "source": [
    "### StandardScaler Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67404205-f40e-4068-ab21-2f73d4a8efdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# StandardScaler\n",
    "scaler = StandardScaler()  # Initialize the scaler\n",
    "X_scaled = scaler.fit_transform(X_example)  # Standardize the features\n",
    "\n",
    "# Step 4: Convert the scaled features back into a DataFrame\n",
    "X_scaled_df = pd.DataFrame(X_scaled, columns=features_digit)\n",
    "\n",
    "# Display the scaled data\n",
    "print(\"Standardized Features (Mean = 0, Std = 1):\")\n",
    "print(X_scaled_df.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77742ab8",
   "metadata": {},
   "source": [
    "### 2.3 Data Transformation Using ColumnTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4085e94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categorical and numerical columns\n",
    "categorical_features = ['type']\n",
    "numeric_features = [col for col in features if col not in categorical_features]\n",
    "\n",
    "# Preprocessing pipeline\n",
    "numeric_transformer =  # Please try to use max_abs_scaler\n",
    "categorical_transformer = OneHotEncoder()\n",
    "\n",
    "# **ColumnTransformer**:\n",
    "# This handles preprocessing for both numerical and categorical features in a single step.\n",
    "# - Numerical data is scaled using `StandardScaler` to normalize values.\n",
    "# - Categorical data (like 'type') is converted into numerical format using one-hot encoding.\n",
    "# Benefits:\n",
    "# - Enables seamless integration of preprocessing into a single pipeline.\n",
    "# - Prevents data leakage by applying transformations only on training data during cross-validation.\n",
    "\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "630ba6ca-3f4a-4526-8435-9c7d4ae3ec91",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a72c9d86-7fb5-4e3f-913a-9406a3aca34d",
   "metadata": {},
   "source": [
    "### Prepare the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49aeb97f-35bc-47e0-b7d5-40b566202709",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Transform the data\n",
    "X_transformed = preprocessor.fit_transform(X)\n",
    "\n",
    "# Train-test split\n",
    "# Step 1: First train-test split to create training and temporary sets\n",
    "# X_transformed: Preprocessed feature data\n",
    "# y: Target labels\n",
    "# test_size=0.3: Reserve 30% of the data for validation and test sets\n",
    "# random_state=42: Ensures reproducibility of the splits\n",
    "# stratify=y: Maintains the class distribution in the split\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "    X_transformed, y, test_size=0.3, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Result:\n",
    "# - X_train, y_train: Training set (70% of the data)\n",
    "# - X_temp, y_temp: Temporary set (30% of the data)\n",
    "\n",
    "# Step 2: Second split to create validation and test sets from the temporary set\n",
    "# test_size=0.5: Splits the remaining 30% of the data equally into validation (15%) and test (15%) sets\n",
    "# random_state=42: Ensures reproducibility\n",
    "# stratify=y_temp: Maintains the class distribution in the split\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42, stratify=y_temp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1655cee9",
   "metadata": {},
   "source": [
    "## Step 3: Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "effbbdcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, roc_auc_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Train Logistic Regression\n",
    "logistic_model = LogisticRegression(max_iter=300, random_state=42)\n",
    "logistic_model.fit(X_train, y_train)\n",
    "\n",
    "y_val_prob_logistic = logistic_model.predict_proba(X_val)[:, 1]\n",
    "auc_val_logistic = roc_auc_score(y_val, y_val_prob_logistic)\n",
    "\n",
    "y_val_pred_logistic = logistic_model.predict(X_val)\n",
    "accuracy_score_logistic = accuracy_score(y_val, y_val_pred_logistic)  # Use predicted labels here\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"Logistic Regression - Validation Performance:\")\n",
    "print(classification_report(y_val, y_val_pred_logistic))\n",
    "print(f\"Validation AUC: {auc_val_logistic:.4f}\")\n",
    "print(f\"Validation accuracy: {accuracy_score_logistic:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "444ac0fd",
   "metadata": {},
   "source": [
    "## Step 4: Multi-Layer Perceptron (MLP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "602ab8bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "# Train MLP Classifier\n",
    "# mlp_model = MLPClassifier(hidden_layer_sizes=(3, 2), activation='relu', solver='adam', max_iter=500, random_state=42)\n",
    "mlp_model = MLPClassifier(hidden_layer_sizes=(3, 2), activation='relu', max_iter=500, random_state=42)\n",
    "mlp_model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate MLP Classifier\n",
    "y_val_pred_mlp = mlp_model.predict(X_val)\n",
    "accuracy_score_mlp = accuracy_score(y_val, y_val_pred_mlp)\n",
    "\n",
    "y_val_prob_mlp = mlp_model.predict_proba(X_val)[:, 1]\n",
    "auc_val_mlp = roc_auc_score(y_val, y_val_prob_mlp)\n",
    "\n",
    "print(\"MLP Classifier - Validation Performance:\")\n",
    "print(classification_report(y_val, y_val_pred_mlp))\n",
    "print(f\"Validation AUC: {auc_val_mlp:.4f}\")\n",
    "print(f\"Validation accuracy: {accuracy_score_mlp:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86b5ac1c",
   "metadata": {},
   "source": [
    "## Step 5: Comparison of Models - Training Loss Curve"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ae23630-0dba-476f-81ad-ab35a8a938db",
   "metadata": {},
   "source": [
    "### Manually record the loss of logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce1b222d-1400-48c9-a45a-0f7a1ad52715",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import log_loss\n",
    "# Initialize the model with warm_start to fit incrementally\n",
    "logistic_model = LogisticRegression(\n",
    "    max_iter=1,  # Run one iteration at a time\n",
    "    solver='saga',  # The saga solver supports recording loss values during optimization\n",
    "    random_state=42,\n",
    "    warm_start=True  # Continue training from the last state, which allow manually record the loss at each iteration\n",
    ")\n",
    "\n",
    "# To store the loss values\n",
    "loss_curve = []\n",
    "\n",
    "# Train the model incrementally\n",
    "for i in range(1, 301):  # 300 iterations\n",
    "    logistic_model.fit(X_train, y_train)  # Fit one iteration\n",
    "    # Predict probabilities to calculate log loss\n",
    "    y_train_prob = logistic_model.predict_proba(X_train)\n",
    "    # Calculate the log loss and append to the loss curve\n",
    "    loss = log_loss(y_train, y_train_prob)\n",
    "    loss_curve.append(loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0633829e-745e-427b-b877-c9069f48c638",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract loss during MLP training\n",
    "mlp_model_loss_curve = mlp_model.loss_curve_\n",
    "Logic_loss = loss_curve\n",
    "\n",
    "# Plot the loss difference during MLP training and Logistic\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(range(1, len(Logic_loss) + 1), Logic_loss, label=\"Logistic Loss Curve\")\n",
    "plt.plot(mlp_model_loss_curve, label=\"MLP Loss Curve\")\n",
    "plt.xlabel(\"Iterations\")\n",
    "plt.ylabel(\"Loss\")\n",
    "yourname = \n",
    "plt.title(yourname+\": Training Loss Over Iterations\")\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc460480-f995-46df-9419-e04606db0062",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "# Plot ROC curves\n",
    "fpr_logistic, tpr_logistic, _ = roc_curve(y_val, y_val_prob_logistic)\n",
    "fpr_mlp, tpr_mlp, _ = roc_curve(y_val, y_val_prob_mlp)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(fpr_logistic, tpr_logistic, label=f\"Logistic Regression (AUC = {auc_val_logistic:.2f})\")\n",
    "plt.plot(fpr_mlp, tpr_mlp, label=f\"MLP Classifier (AUC = {auc_val_mlp:.2f})\")\n",
    "plt.title(\"ROC Curve Comparison\")\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4878c82",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "- Logistic Regression is a simpler model but might perform comparably on well-processed datasets.\n",
    "- MLP leverages neural network architecture, potentially achieving better results for complex data.\n",
    "- AUC-ROC curves are helpful for comparing models' classification performance at different thresholds."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63f17bd0",
   "metadata": {},
   "source": [
    "## Step 6: Visualization of Predictions and Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23617576",
   "metadata": {},
   "source": [
    "### 6.1 Actual vs Predicted Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eedafb4a-13ed-4d69-bd32-8e0204018137",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(range(len(y_val)), y_val, color='blue', alpha=0.6, label=\"Actual Values\", s=15)\n",
    "plt.title(\"Actual results\")\n",
    "plt.xlabel(\"Sample Index\")\n",
    "plt.ylabel(\"Legendary (1) or Not (0)\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a1d2a5f-c29e-497b-ba0c-331dd015634f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.scatter(range(len(y_val)), y_val_pred_logistic, color='red', alpha=0.6, label=\"Predicted (Logistic Regression)\", s=15)\n",
    "plt.title(\"<Your Name> +  Predicted (Logistic Regression)\")\n",
    "plt.xlabel(\"Sample Index\")\n",
    "plt.ylabel(\"Legendary (1) or Not (0)\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5f00bd4-db72-4ad8-8665-230f4915405a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(range(len(y_val)), y_val_pred_mlp, color='green', alpha=0.6, label=\"Predicted (MLP)\", s=15)\n",
    "plt.title(\"Predicted (MLP)\")\n",
    "plt.xlabel(\"Sample Index\")\n",
    "plt.ylabel(\"Legendary (1) or Not (0)\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57bbc029-fb64-4d12-9d55-aa8d9f9f496c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Identify error indices for Logistic Regression\n",
    "errors_logistic = (y_val != y_val_pred_logistic)\n",
    "\n",
    "# Identify error indices for MLP\n",
    "errors_mlp = (y_val != y_val_pred_mlp)\n",
    "\n",
    "# Plot the actual values at error indices\n",
    "plt.scatter(\n",
    "    np.where(errors_logistic)[0],  # Indices of errors for Logistic Regression\n",
    "    y_val[errors_logistic],  # Actual values where Logistic Regression fails\n",
    "    color='blue', alpha=0.6, label=\"Actual Values (Errors for Logistic Regression)\", s=15\n",
    ")\n",
    "\n",
    "# # Plot the Logistic Regression predictions at error indices\n",
    "plt.scatter(\n",
    "    np.where(errors_logistic)[0],  # Indices of errors for Logistic Regression\n",
    "    y_val_pred_logistic[errors_logistic],  # Predicted values where errors occur\n",
    "    color='red', alpha=0.6, label=\"Predicted (Logistic Regression Errors)\", s=15\n",
    ")\n",
    "\n",
    "# # Plot the MLP predictions at error indices\n",
    "plt.scatter(\n",
    "    np.where(errors_mlp)[0],  # Indices of errors for MLP\n",
    "    y_val_pred_mlp[errors_mlp],  # Predicted values where errors occur\n",
    "    color='green', alpha=0.6, label=\"Predicted (MLP Errors)\", s=15\n",
    ")\n",
    "\n",
    "# Add plot details\n",
    "plt.xlabel(\"Index\")\n",
    "plt.ylabel(\"Values\")\n",
    "plt.title(\"<Your Name> +  Error Points for Logistic Regression and MLP\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c130bacc-cf92-4d5b-afeb-c60d16363459",
   "metadata": {},
   "source": [
    "### Confusion Matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a3da9ce-c38d-4982-b238-61e348f95855",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "# Logistic Regression\n",
    "cm_logistic = confusion_matrix(y_val, y_val_pred_logistic)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm_logistic, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
    "plt.title(\"<Your Name> + Confusion Matrix: Logistic Regression\")\n",
    "plt.xlabel(\"Predicted Label\")\n",
    "plt.ylabel(\"True Label\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdadff2d-a5bd-4bb2-b166-5e151a2bd17a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLP\n",
    "cm_mlp = confusion_matrix(y_val, y_val_pred_mlp)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm_mlp, annot=True, fmt=\"d\", cmap=\"Greens\")\n",
    "plt.title(\"<Your Name> + Confusion Matrix: MLP Classifier\")\n",
    "plt.xlabel(\"Predicted Label\")\n",
    "plt.ylabel(\"True Label\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b87cf8d-0856-4bcd-b102-1ac9e8f22f36",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
